# -*- coding: utf-8 -*-
"""
Spyder Editor

This is a temporary script file.
"""

import pandas as pd
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.sentiment.sentiment_analyzer import SentimentAnalyzer
from nltk.sentiment.vader import SentimentIntensityAnalyzer

ignore_words = stopwords.words('english')
senti_obj =  SentimentIntensityAnalyzer()

data = pd.read_csv(r'E:\DjangoProject\SecFilings\sec\deepak_10k.csv', names=['Header', 'Text', 'PageNumber'])

data.Header = data.Header.apply(lambda x: x.replace('Â', '').replace('Table of Contents', '') if isinstance(x, str) else x)
data.Text = data.Text.apply(lambda x: x.replace('Â', '').replace('Table of Contents', '') if isinstance(x, str) else x)

def _replace(x):
    if isinstance(x, str):
        if len(x) < 25:
            return
        elif x.count('$') > 2:
            return
        elif x.count('_') > 3:
            return
        
        return x
    return x

data.Text = data.Text.apply(_replace)
data.dropna(inplace=True)
data.PageNumber = data.PageNumber.apply(lambda x: int(x) + 1)
#
#def sentimental(x):
#    return senti_obj.polarity_scores(' '.join([i for i in word_tokenize(x) if i not in ignore_words]))
#
#data['deepak_1'] = data.Header.apply(sentimental)
#
#data['deepak_2'] = data.Text.apply(sentimental)
#
# Nltk: get entities of the words
#def extract_entities(text):
#    for sent in nltk.sent_tokenize(text):
#        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):
#            if hasattr(chunk, 'label'):
#                print(text, '\n', chunk.label(), '---', chunk, ' '.join(c[0] for c in chunk.leaves()))
#
#data.Header.apply(extract_entities)

import spacy
from spacy import displacy
from collections import Counter
from pprint import pprint

nlp = spacy.load('en_core_web_sm')


#def entries(text):
#    doc = nlp(text) 
#    ents1 = [(X.text, X.label_) for X in doc.ents]
#    print(ents1, type(ents1))
#    
#data.Header.apply(entries)



doc = nlp('European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices')
pprint([(X.text, X.label_) for X in doc.ents])
sentences = [x for x in doc.sents]
displacy.render(nlp(str(sentences[0])), style='ent')




#########show into the html ###################
import spacy
from spacy import displacy

text = """But Google is starting from behind. The company made a late push
into hardware, and Apple’s Siri, available on iPhones, and Amazon’s Alexa
software, which runs on its Echo and Dot devices, have clear leads in
consumer adoption."""

nlp = spacy.load('en_core_web_sm')
doc = nlp(text)
displacy.serve(doc, style='ent')

